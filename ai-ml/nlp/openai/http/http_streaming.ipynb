{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "TIMEOUT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def openai_stream(\n",
    "    data: dict,\n",
    ") -> AsyncGenerator[str, None]:\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        async with client.stream(\n",
    "            \"POST\",\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            timeout=httpx.Timeout(TIMEOUT),\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "            },\n",
    "            json=data,\n",
    "        ) as response:\n",
    "            print(f\"received response status_code={response.status_code}\")\n",
    "            response.raise_for_status()\n",
    "            async for chunk in response.aiter_text():\n",
    "                yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncGenerator\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ResponseMessage(BaseModel):\n",
    "    content: str\n",
    "\n",
    "\n",
    "async def _handle_function_call(name: str, arguments: dict) -> str:\n",
    "    pass\n",
    "\n",
    "\n",
    "async def response_generator(user_message: str) -> AsyncGenerator[str, None]:\n",
    "    func_call = {\"arguments\": \"\", \"name\": None}\n",
    "    async for response in openai_stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"message\": user_message},\n",
    "            ],\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"stream\": True,\n",
    "        }\n",
    "    ):\n",
    "        for block_raw in response.split(\"\\n\\n\"):\n",
    "            for line in block_raw.split(\"\\n\"):\n",
    "                if line.startswith(\"data:\"):\n",
    "                    json_str = line.replace(\"data:\", \"\").strip()\n",
    "                    if json_str == \"[DONE]\":\n",
    "                        break\n",
    "                    else:\n",
    "                        block = json.loads(json_str)\n",
    "\n",
    "                        # we assume that we only need to look at the first choice\n",
    "                        choice = block[\"choices\"][0]\n",
    "                        delta = choice.get(\"delta\")\n",
    "                    if \"function_call\" in delta:\n",
    "                        name = delta[\"function_call\"].get(\"name\")\n",
    "                        if name:\n",
    "                            func_call[\"name\"] = name\n",
    "                        arguments = delta[\"function_call\"].get(\"arguments\")\n",
    "                        if arguments:\n",
    "                            func_call[\"arguments\"] += arguments\n",
    "                    elif \"content\" in delta:\n",
    "                        yield ResponseMessage(\n",
    "                            content=delta[\"content\"],\n",
    "                        ).model_dump_json() + \"\\n\"\n",
    "\n",
    "    # we only handle the function call once all the data has been streamed in\n",
    "    if func_call[\"name\"] is not None:\n",
    "        response_message = await _handle_function_call(\n",
    "            func_call[\"name\"],\n",
    "            func_call[\"arguments\"],\n",
    "        )\n",
    "        yield ResponseMessage(\n",
    "            content=response_message,\n",
    "        ).model_dump_json() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class RequestMessage(BaseModel):\n",
    "    message: str\n",
    "\n",
    "@app.post(\"/\")\n",
    "async def message(\n",
    "    request: RequestMessage,\n",
    "):\n",
    "    return StreamingResponse(\n",
    "        response_generator(request.message),\n",
    "        media_type=\"application/x-ndjson\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
