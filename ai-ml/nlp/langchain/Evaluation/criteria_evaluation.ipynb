{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\")\n",
    "\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nLooking at the submission, the answer to the question \"What\\'s 2 + 2?\" is indeed \"four\". However, the respondent included additional information that was not necessary to answer the question, such as \"That\\'s an elementary question\" and \"The answer you\\'re looking for is that two and two is four.\"\\n\\nBased on this, the submission is not concise as it includes unnecessary information. \\n\\nTherefore, the submission does not meet the criterion of conciseness.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"What's 2 + 2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
    "    input=\"What's 2 + 2?\",\n",
    ")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LabeledCriteriaEvalChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
      "[BEGIN DATA]\n",
      "***\n",
      "[Input]: What is the capital of the US?\n",
      "***\n",
      "[Submission]: Topeka, KS\n",
      "***\n",
      "[Criteria]: correctness: Is the submission correct, accurate, and factual?\n",
      "***\n",
      "[Reference]: The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023\n",
      "***\n",
      "[END DATA]\n",
      "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "With ground truth: 1\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\n",
    "\n",
    "# We can even override the model's learned knowledge using ground truth labels\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"What is the capital of the US?\",\n",
    "    prediction=\"Topeka, KS\",\n",
    "    reference=\"The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023\",\n",
    ")\n",
    "print(f'With ground truth: {eval_result[\"score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "\n",
    "# For a list of other default supported criteria, try calling `supported_default_criteria`\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new CriteriaEvalChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
      "[BEGIN DATA]\n",
      "***\n",
      "[Input]: Tell me a joke\n",
      "***\n",
      "[Submission]: I ate some square pie but I don't know the square of pi.\n",
      "***\n",
      "[Criteria]: numeric: Does the output contain numeric or mathematical information?\n",
      "***\n",
      "[END DATA]\n",
      "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'reasoning': 'The criterion asks if the output contains numeric or mathematical information. \\n\\nThe submission is a joke that says, \"I ate some square pie but I don\\'t know the square of pi.\"\\n\\nIn this joke, there is a reference to the mathematical term \"square\" and the mathematical constant \"pi\". \\n\\nTherefore, the submission does contain numeric or mathematical information, and it meets the criterion.\\n\\nY', 'value': 'Y', 'score': 1}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CriteriaEvalChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
      "[BEGIN DATA]\n",
      "***\n",
      "[Input]: Tell me a joke\n",
      "***\n",
      "[Submission]: I ate some square pie but I don't know the square of pi.\n",
      "***\n",
      "[Criteria]: numeric: Does the output contain numeric information?\n",
      "mathematical: Does the output contain mathematical information?\n",
      "grammatical: Is the output grammatically correct?\n",
      "logical: Is the output logical?\n",
      "***\n",
      "[END DATA]\n",
      "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Multi-criteria evaluation\n",
      "{'reasoning': 'Let\\'s assess the submission based on the given criteria:\\n\\n1. Numeric: The output does not contain any explicit numeric information. The word \"square\" and \"pi\" are used, but no actual numbers are given.\\n\\n2. Mathematical: The output does contain mathematical information. The terms \"square\" and \"pi\" are mathematical terms, and the concept of squaring a number is a mathematical operation.\\n\\n3. Grammatical: The output is grammatically correct. The sentence structure is sound and all words are used appropriately.\\n\\n4. Logical: The output is logical. It\\'s a joke that plays on the words \"square pie\" and \"square of pi\", which makes sense in the context of the input.\\n\\nBased on this analysis, the submission does not meet all the criteria because it does not contain numeric information.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "custom_criterion = {\n",
    "    \"numeric\": \"Does the output contain numeric or mathematical information?\"\n",
    "}\n",
    "\n",
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criterion,\n",
    ")\n",
    "query = \"Tell me a joke\"\n",
    "prediction = \"I ate some square pie but I don't know the square of pi.\"\n",
    "eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query)\n",
    "print(eval_result)\n",
    "\n",
    "# If you wanted to specify multiple criteria. Generally not recommended\n",
    "custom_criteria = {\n",
    "    \"numeric\": \"Does the output contain numeric information?\",\n",
    "    \"mathematical\": \"Does the output contain mathematical information?\",\n",
    "    \"grammatical\": \"Is the output grammatically correct?\",\n",
    "    \"logical\": \"Is the output logical?\",\n",
    "}\n",
    "\n",
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criteria,\n",
    ")\n",
    "eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query)\n",
    "print(\"Multi-criteria evaluation\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 available principles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('harmful1',\n",
       "  ConstitutionalPrinciple(critique_request='Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.', revision_request='Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.', name='harmful1')),\n",
       " ('harmful2',\n",
       "  ConstitutionalPrinciple(critique_request='Explain ways in which the assistant’s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.', revision_request='Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.', name='harmful2')),\n",
       " ('harmful3',\n",
       "  ConstitutionalPrinciple(critique_request='Identify all ways in which the assistant’s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.', revision_request='Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.', name='harmful3')),\n",
       " ('harmful4',\n",
       "  ConstitutionalPrinciple(critique_request='Identify specific ways in which the assistant’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.', revision_request='Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.', name='harmful4')),\n",
       " ('insensitive',\n",
       "  ConstitutionalPrinciple(critique_request='Discuss the assistant’s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.', revision_request='Revise the assistant’s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.', name='insensitive'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.constitutional_ai.principles import PRINCIPLES\n",
    "\n",
    "print(f\"{len(PRINCIPLES)} available principles\")\n",
    "list(PRINCIPLES.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
