{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\18353\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\18353\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Model Keywords:\n",
      "Topic 0: ., language, python, programming, guido\n",
      "Topic 1: ., programming, python, language, ,\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mword,\u001b[38;5;250m \u001b[39mprob\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mkeywords])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# 主题模型关键词\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m topic_keywords \u001b[38;5;241m=\u001b[39m \u001b[43mget_topic_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_keywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic Model Keywords:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic, keywords \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(topic_keywords):\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mget_topic_keywords\u001b[1;34m(corpus, num_topics, num_keywords)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_keywords\u001b[39m(corpus, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# 创建字典\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     dictionary \u001b[38;5;241m=\u001b[39m \u001b[43mcorpora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# 创建语料库\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m corpus]\n",
      "File \u001b[1;32mc:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nnz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprune_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprune_at\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    201\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc2bow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[0;32m    206\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos)\n",
      "File \u001b[1;32mc:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:241\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(document, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc2bow expects an array of unicode tokens on input, not a single string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Construct (word, frequency) mapping.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m counter \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "\n",
    "# 下载 NLTK 数据\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "def preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# 主题模型关键词抽取\n",
    "def get_topic_keywords(corpus, num_topics=5, num_keywords=10):\n",
    "    # 创建字典\n",
    "    dictionary = corpora.Dictionary(corpus)\n",
    "    \n",
    "    # 创建语料库\n",
    "    corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "    \n",
    "    # 训练 LDA 模型\n",
    "    ldamodel = gensim.models.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "    \n",
    "    # 获取每个主题的关键词\n",
    "    topic_keywords = [[(dictionary[id], prob) for id, prob in ldamodel.get_topic_terms(topicid, topn=num_keywords)]\n",
    "                      for topicid in range(ldamodel.num_topics)]\n",
    "    \n",
    "    return topic_keywords\n",
    "\n",
    "# BERT 关键词抽取\n",
    "def get_bert_keywords(texts, model, tokenizer, max_len=512):\n",
    "    all_keywords = []\n",
    "    for text in texts:\n",
    "        # 对文本进行编码\n",
    "        encoding = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_len, \n",
    "                                         return_tensors='pt', truncation=True)\n",
    "        \n",
    "        # 获取输出\n",
    "        output = model(encoding['input_ids'], encoding['attention_mask'])[0]\n",
    "        \n",
    "        # 获取关键词索引\n",
    "        keyword_indices = torch.where(output[0] > 0)[0].tolist()\n",
    "        \n",
    "        # 解码关键词\n",
    "        keywords = tokenizer.convert_ids_to_tokens([encoding['input_ids'][0][i] for i in keyword_indices])\n",
    "        \n",
    "        all_keywords.append(keywords)\n",
    "        \n",
    "    return all_keywords\n",
    "\n",
    "# 示例用法\n",
    "corpus = [\n",
    "    \"This is a sample document about Python programming language.\",\n",
    "    \"Python is a high-level, general-purpose programming language.\",\n",
    "    \"It was created by Guido van Rossum in the late 1980s.\"\n",
    "]\n",
    "\n",
    "# 对文本进行预处理\n",
    "preprocessed_corpus = [preprocess(text) for text in corpus]\n",
    "\n",
    "# 主题模型关键词\n",
    "topic_keywords = get_topic_keywords(preprocessed_corpus, num_topics=2, num_keywords=5)\n",
    "print(\"Topic Model Keywords:\")\n",
    "for topic, keywords in enumerate(topic_keywords):\n",
    "    print(f\"Topic {topic}: {', '.join([word for word, prob in keywords])}\")\n",
    "\n",
    "# 主题模型关键词\n",
    "topic_keywords = get_topic_keywords(corpus, num_topics=2, num_keywords=5)\n",
    "print(\"Topic Model Keywords:\")\n",
    "for topic, keywords in enumerate(topic_keywords):\n",
    "    print(f\"Topic {topic}: {', '.join([word for word, prob in keywords])}\")\n",
    "\n",
    "# BERT 关键词抽取\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n",
    "bert_keywords = get_bert_keywords(corpus, model, tokenizer)\n",
    "print(\"\\nBERT Keywords:\")\n",
    "for text, keywords in zip(corpus, bert_keywords):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Keywords: {', '.join(keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
