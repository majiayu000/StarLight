{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'shared_articles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m articles_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshared_articles.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(articles_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'shared_articles.csv'"
     ]
    }
   ],
   "source": [
    "articles_df = pd.read_csv('shared_articles.csv')\n",
    "print(articles_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\learn\\star\\StarLight\\MachineLearning\\recommendation\\simple-art\\data\\essay\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "target_dir = os.path.join(base_dir, 'data\\essay')\n",
    "\n",
    "print(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = []\n",
    "\n",
    "# 读取目标文件夹内所有文件的内容\n",
    "for file in os.listdir(target_dir):\n",
    "    with open(os.path.join(target_dir, file), 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        cleaned_text = content.replace(\"[wave]\", \"\")\n",
    "        parts = content.split('\\n\\n', 1) \n",
    "        \n",
    "        # 最多分割成两部分\n",
    "        if len(parts) > 1:\n",
    "            title = parts[0].replace(\"Title: \", \"\").strip()  # 移除\"Title: \"部分并去除两端空白字符\n",
    "            content = parts[1]  # 内容是第二部分\n",
    "        else:\n",
    "            title = None  # 如果没有找到分隔符，那么可能没有标题\n",
    "            content = parts[0]\n",
    "        essays.append({ 'title': title, 'content': content})\n",
    "\n",
    "# 将列表转换为DataFrame\n",
    "df = pd.DataFrame(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>地球妈妈，我想对你说</td>\n",
       "      <td>地球妈妈，我想对你说：“我昨天做了一个梦，梦见‘宇宙急诊室’里有一个病情十分严重的患者，只见...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>想飞的蚂蚁</td>\n",
       "      <td>[wave]蔚蓝的天空中，漂浮着几朵洁白的云，三五成群的鸟儿在云朵里捉着迷藏，一行大雁排着整...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我最敬佩的一个人</td>\n",
       "      <td>我最敬佩的一个人是一位环卫工人。他高瘦的身材，圆圆的眼睛，黑黑的脸庞，衣衫褴褛，别看它的外表...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>夜空</td>\n",
       "      <td>[wave]天，渐渐的暗了下来。太阳也收敛它最后的笑容。夜神用它那深蓝的袍子把整个天空都笼罩...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>秋菊</td>\n",
       "      <td>早晨，当你踏入校园的第一步时，你会感觉到，一阵清香扑鼻而来，紧接着，一片[wave]红花衬绿...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                                            content\n",
       "0  地球妈妈，我想对你说  地球妈妈，我想对你说：“我昨天做了一个梦，梦见‘宇宙急诊室’里有一个病情十分严重的患者，只见...\n",
       "1       想飞的蚂蚁  [wave]蔚蓝的天空中，漂浮着几朵洁白的云，三五成群的鸟儿在云朵里捉着迷藏，一行大雁排着整...\n",
       "2    我最敬佩的一个人  我最敬佩的一个人是一位环卫工人。他高瘦的身材，圆圆的眼睛，黑黑的脸庞，衣衫褴褛，别看它的外表...\n",
       "3          夜空  [wave]天，渐渐的暗了下来。太阳也收敛它最后的笑容。夜神用它那深蓝的袍子把整个天空都笼罩...\n",
       "4          秋菊  早晨，当你踏入校园的第一步时，你会感觉到，一阵清香扑鼻而来，紧接着，一片[wave]红花衬绿..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(x):\n",
    "    soup = ' '.join(x['content'])\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soup'] = df.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>soup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>地球妈妈，我想对你说</td>\n",
       "      <td>地球妈妈，我想对你说：“我昨天做了一个梦，梦见‘宇宙急诊室’里有一个病情十分严重的患者，只见...</td>\n",
       "      <td>地 球 妈 妈 ， 我 想 对 你 说 ： “ 我 昨 天 做 了 一 个 梦 ， 梦 见 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                                            content  \\\n",
       "0  地球妈妈，我想对你说  地球妈妈，我想对你说：“我昨天做了一个梦，梦见‘宇宙急诊室’里有一个病情十分严重的患者，只见...   \n",
       "\n",
       "                                                soup  \n",
       "0  地 球 妈 妈 ， 我 想 对 你 说 ： “ 我 昨 天 做 了 一 个 梦 ， 梦 见 ...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>soup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>地球妈妈，我想对你说</td>\n",
       "      <td>地球妈妈，我想对你说：“我昨天做了一个梦，梦见‘宇宙急诊室’里有一个病情十分严重的患者，只见...</td>\n",
       "      <td>地 球 妈 妈 ， 我 想 对 你 说 ： “ 我 昨 天 做 了 一 个 梦 ， 梦 见 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>想飞的蚂蚁</td>\n",
       "      <td>[wave]蔚蓝的天空中，漂浮着几朵洁白的云，三五成群的鸟儿在云朵里捉着迷藏，一行大雁排着整...</td>\n",
       "      <td>[ w a v e ] 蔚 蓝 的 天 空 中 ， 漂 浮 着 几 朵 洁 白 的 云 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我最敬佩的一个人</td>\n",
       "      <td>我最敬佩的一个人是一位环卫工人。他高瘦的身材，圆圆的眼睛，黑黑的脸庞，衣衫褴褛，别看它的外表...</td>\n",
       "      <td>我 最 敬 佩 的 一 个 人 是 一 位 环 卫 工 人 。 他 高 瘦 的 身 材 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>夜空</td>\n",
       "      <td>[wave]天，渐渐的暗了下来。太阳也收敛它最后的笑容。夜神用它那深蓝的袍子把整个天空都笼罩...</td>\n",
       "      <td>[ w a v e ] 天 ， 渐 渐 的 暗 了 下 来 。 太 阳 也 收 敛 它 最 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>秋菊</td>\n",
       "      <td>早晨，当你踏入校园的第一步时，你会感觉到，一阵清香扑鼻而来，紧接着，一片[wave]红花衬绿...</td>\n",
       "      <td>早 晨 ， 当 你 踏 入 校 园 的 第 一 步 时 ， 你 会 感 觉 到 ， 一 阵 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                                            content  \\\n",
       "0  地球妈妈，我想对你说  地球妈妈，我想对你说：“我昨天做了一个梦，梦见‘宇宙急诊室’里有一个病情十分严重的患者，只见...   \n",
       "1       想飞的蚂蚁  [wave]蔚蓝的天空中，漂浮着几朵洁白的云，三五成群的鸟儿在云朵里捉着迷藏，一行大雁排着整...   \n",
       "2    我最敬佩的一个人  我最敬佩的一个人是一位环卫工人。他高瘦的身材，圆圆的眼睛，黑黑的脸庞，衣衫褴褛，别看它的外表...   \n",
       "3          夜空  [wave]天，渐渐的暗了下来。太阳也收敛它最后的笑容。夜神用它那深蓝的袍子把整个天空都笼罩...   \n",
       "4          秋菊  早晨，当你踏入校园的第一步时，你会感觉到，一阵清香扑鼻而来，紧接着，一片[wave]红花衬绿...   \n",
       "\n",
       "                                                soup  \n",
       "0  地 球 妈 妈 ， 我 想 对 你 说 ： “ 我 昨 天 做 了 一 个 梦 ， 梦 见 ...  \n",
       "1  [ w a v e ] 蔚 蓝 的 天 空 中 ， 漂 浮 着 几 朵 洁 白 的 云 ， ...  \n",
       "2  我 最 敬 佩 的 一 个 人 是 一 位 环 卫 工 人 。 他 高 瘦 的 身 材 ， ...  \n",
       "3  [ w a v e ] 天 ， 渐 渐 的 暗 了 下 来 。 太 阳 也 收 敛 它 最 ...  \n",
       "4  早 晨 ， 当 你 踏 入 校 园 的 第 一 步 时 ， 你 会 感 觉 到 ， 一 阵 ...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = []\n",
    "\n",
    "with open(\"baidu_stopwords.txt\", encoding='utf-8') as f:\n",
    "    stop_content = f.read()\n",
    "stop_word_list = stop_content.strip().split('\\n')\n",
    "stop_word_list.extend(['，', '。', '、', '；', '：', '？', '！', '“', '”', '（', '）', '《', '》', '【', '】'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def chinese_tokenizer(text):\n",
    "    return jieba.lcut(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'a', 'ain', 'aren', 'c', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'i', 'isn', 'll', 'm', 'mon', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn', '下', '不', '为什', '什', '今', '使', '先', '却', '只', '唷', '啪', '喔', '天', '好', '後', '最', '漫', '然', '特', '特别', '见', '设', '说', '达', '面', '麽', '－'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=chinese_tokenizer, stop_words=stop_word_list)\n",
    "\n",
    "# 对文本数据进行分词和向量化处理\n",
    "tfidf_matrix = tfidf.fit_transform(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\t', '\\n', ' ', ..., '龟粮', '＂', '．'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_array = tfidf.get_feature_names_out()\n",
    "feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 top 10 keywords: ['大夫' '‘' '’' '急诊室' '火星' '人类' '面临' '大部分' '太阳' '母亲']\n",
      "Document 2 top 10 keywords: ['小蚂蚁' '飞' '试验' '绑' '螺旋桨' '资料' '[' ']' '树叶' '翅膀']\n",
      "Document 3 top 10 keywords: ['消毒器' '竹枝' '敬佩' '喷喷' '衣衫褴褛' '海尔' '毒药' '评为' '蛆' '捐给']\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "for i, doc in enumerate(df):\n",
    "    tfidf_sorting = tfidf_matrix[i].toarray()[0].argsort()[::-1]\n",
    "    top_n = feature_array[tfidf_sorting][:N]\n",
    "    print(f\"Document {i+1} top {N} keywords: {top_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "# 保存TF-IDF矩阵\n",
    "save_npz('tfidf_matrix.npz', tfidf_matrix)\n",
    "\n",
    "# 需要时可以这样重新加载\n",
    "tfidf_matrix_loaded = load_npz('tfidf_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate memory usage: 355216 bytes\n",
      "Approximate memory usage: 346.89 KB\n",
      "Approximate memory usage: 0.34 MB\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "\n",
    "# 加载矩阵\n",
    "tfidf_matrix_loaded = load_npz('tfidf_matrix.npz')\n",
    "\n",
    "# 计算内存使用\n",
    "# 注意：这个估计假设所有的数值都以64位浮点数存储。如果使用的是其他类型，需要相应调整。\n",
    "memory_usage = (\n",
    "    tfidf_matrix_loaded.data.nbytes + \n",
    "    tfidf_matrix_loaded.indptr.nbytes + \n",
    "    tfidf_matrix_loaded.indices.nbytes\n",
    ")\n",
    "\n",
    "print(f\"Approximate memory usage: {memory_usage} bytes\")\n",
    "\n",
    "# 转换为更可读的单位\n",
    "memory_usage_kb = memory_usage / 1024\n",
    "memory_usage_mb = memory_usage_kb / 1024\n",
    "\n",
    "print(f\"Approximate memory usage: {memory_usage_kb:.2f} KB\")\n",
    "print(f\"Approximate memory usage: {memory_usage_mb:.2f} MB\")\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 10514)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 10514)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['挽' '挽留' '挽起' '捂住' '捂着' '捉' '捉住' '捉弄' '捉虫' '捉迷藏']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.get_feature_names_out()[5000:5010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.03495535, 0.0188519 , ..., 0.0256755 , 0.03733507,\n",
       "        0.01328   ],\n",
       "       [0.03495535, 1.        , 0.07068963, ..., 0.09122872, 0.07013314,\n",
       "        0.06046115],\n",
       "       [0.0188519 , 0.07068963, 1.        , ..., 0.06705289, 0.0445332 ,\n",
       "        0.02604968],\n",
       "       ...,\n",
       "       [0.0256755 , 0.09122872, 0.06705289, ..., 1.        , 0.09754205,\n",
       "        0.07847147],\n",
       "       [0.03733507, 0.07013314, 0.0445332 , ..., 0.09754205, 1.        ,\n",
       "        0.06286133],\n",
       "       [0.01328   , 0.06046115, 0.02604968, ..., 0.07847147, 0.06286133,\n",
       "        1.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix, True)\n",
    "display(cosine_sim.shape)\n",
    "display(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "地球妈妈，我想对你说      0\n",
       "想飞的蚂蚁           1\n",
       "我最敬佩的一个人        2\n",
       "夜空              3\n",
       "秋菊              4\n",
       "寻找春天            5\n",
       "落叶              6\n",
       "《和好孩子交朋友》读后感    7\n",
       "感谢              8\n",
       "小蚂蚁得救了          9\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = df.reset_index()\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "display(indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, indices, cosine_sim, data):\n",
    "    # Get the index of the article that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all articles with that article\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the articles based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the article indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar articles\n",
    "    return data['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151    假如（老师）\n",
      "283        孩子\n",
      "72        中秋节\n",
      "238     最美丽的梦\n",
      "225    星星里的小人\n",
      "244         爱\n",
      "266        青春\n",
      "227      那些曾经\n",
      "292      一年四季\n",
      "131     春天在哪里\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(get_recommendations('夜空', indices, cosine_sim\n",
    "                            , metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_by_idx(idx, cosine_sim, data, top_n=10, sim_threshold=0.1):\n",
    "    # 保证索引在cosine_sim的范围内\n",
    "    if idx >= len(cosine_sim):\n",
    "        return \"Index out of bounds.\"\n",
    "    \n",
    "    # 获取所有文章与该索引文章的相似度得分\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # 根据相似度得分排序文章，并移除自身\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "\n",
    "    # 应用相似度阈值过滤\n",
    "    sim_scores = [sim_score for sim_score in sim_scores if sim_score[1] > sim_threshold]\n",
    "\n",
    "    # 获取最相似文章的索引\n",
    "    article_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # 返回最相似文章的标题\n",
    "    recommendations = data['title'].iloc[article_indices]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# 示例调用\n",
    "# 假设你已经通过某种方式找到了与关键词最相关的文章索引article_idx\n",
    "# cosine_sim是文章相似度矩阵，data是包含文章标题的DataFrame\n",
    "# recommendations = get_recommendations_by_idx(article_idx, cosine_sim, data, top_n=10, sim_threshold=0.2)\n",
    "# print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n",
      "244    爱\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 假设df是你的DataFrame，包含了'content'和'title'列\n",
    "# 以下示例创建一个基于内容的TF-IDF矩阵，并基于内容的相似度来进行推荐\n",
    "\n",
    "# 创建TF-IDF向量化器\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_word_list)\n",
    "\n",
    "# 向量化文章内容\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['content'])\n",
    "\n",
    "# 计算余弦相似度矩阵\n",
    "cosine_sim_content = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 假设有一个查询函数，用于找到与查询关键词最相关的文章\n",
    "def find_article_by_keyword(keyword, tfidf_vectorizer, df):\n",
    "    # 将查询关键词转换为TF-IDF向量\n",
    "    keyword_vector = tfidf_vectorizer.transform([keyword])\n",
    "    \n",
    "    # 计算查询关键词与所有文章内容的相似度\n",
    "    sim_scores_keyword = cosine_similarity(keyword_vector, tfidf_matrix)\n",
    "    \n",
    "    # 找到最相似的文章索引\n",
    "    most_similar_article_idx = sim_scores_keyword.argsort()[0][-1]\n",
    "    \n",
    "    return most_similar_article_idx\n",
    "\n",
    "# 假设关键词是\"user experience\"\n",
    "keyword = \"夜空\"\n",
    "article_idx = find_article_by_keyword(keyword, tfidf_vectorizer, df)\n",
    "print(article_idx)\n",
    "\n",
    "# 使用上述`get_recommendations`函数获取推荐\n",
    "# 注意，需要稍作修改以接受索引而非标题作为输入\n",
    "recommendations = get_recommendations_by_idx(article_idx, cosine_sim_content, df)\n",
    "\n",
    "# 打印推荐结果的标题\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, indices, cosine_sim, data, top_n=10, sim_threshold=0.1):\n",
    "    # 检查提供的标题是否存在于索引中\n",
    "    if title not in indices:\n",
    "        return f\"Title '{title}' not found.\"\n",
    "\n",
    "    # 获取文章的索引\n",
    "    idx = indices[title]\n",
    "\n",
    "    # 获取所有文章与该文章的相似度得分\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # 根据相似度得分排序文章，并移除自身\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "\n",
    "    # 应用相似度阈值过滤\n",
    "    sim_scores = [sim_score for sim_score in sim_scores if sim_score[1] > sim_threshold]\n",
    "\n",
    "    # 获取最相似文章的索引\n",
    "    article_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # 获取最相似文章的标题\n",
    "    recommendations = data['title'].iloc[article_indices]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (1, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 定义中文停用词集合\n",
    "chinese_stop_words = {'的', '了', '在', '是', '我', '有', '和', '就'}\n",
    "chinese_stop_words_list = list(chinese_stop_words)\n",
    "\n",
    "# 使用转换后的列表创建TfidfVectorizer实例\n",
    "tfidf = TfidfVectorizer(stop_words=chinese_stop_words_list)\n",
    "\n",
    "# 示例：使用TfidfVectorizer实例\n",
    "# 这里只是示例，实际上你需要传入真实的中文文本数据进行向量化\n",
    "texts = [\"我爱北京天安门\", \"天安门上太阳升\"]\n",
    "tfidf_matrix = tfidf.fit_transform(texts)\n",
    "\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_tokenize(text):\n",
    "    return [word for word in jieba.cut(text) if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\18353\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.756 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.48693426407352264\n",
      "  (0, 2)\t0.6176143709756019\n",
      "  (0, 7)\t0.6176143709756019\n",
      "  (1, 4)\t0.6176143709756019\n",
      "  (1, 0)\t0.6176143709756019\n",
      "  (1, 3)\t0.48693426407352264\n",
      "  (2, 6)\t0.7071067811865476\n",
      "  (2, 1)\t0.7071067811865476\n",
      "  (3, 5)\t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18353\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'a', 'ain', 'aren', 'c', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'i', 'isn', 'll', 'm', 'mon', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn', '下', '不', '为什', '什', '今', '使', '先', '却', '只', '唷', '啪', '喔', '天', '好', '後', '最', '漫', '然', '特', '特别', '见', '设', '说', '达', '面', '麽', '－'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=jieba_tokenize, stop_words=stop_word_list)\n",
    "\n",
    "# 准备一些中文文本进行测试\n",
    "texts = [\n",
    "    \"我爱北京天安门\",\n",
    "    \"天安门上太阳升\",\n",
    "    \"伟大领袖毛主席\",\n",
    "    \"指引我们向前进\"\n",
    "]\n",
    "\n",
    "# 使用fit_transform方法进行分词和向量化\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "print(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
